{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Stress Detection on WESAD\n",
    "\n",
    "In this final notebook you will be guided through the steps to build a simple ML classifier and train it on the ECG features you previously computed. This notebook comprises of 2 parts:\n",
    "\n",
    "1. Experimental Setup, data loading, classifier initialization\n",
    "2. Model training, validation, and discussion of results\n",
    "\n",
    "Throughout this process we will make use of the ``scikit-learn`` library that streamlines simple ML experiments. Let's start by loading our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load features and labels files\n",
    "features = ...\n",
    "labels = ...\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may recall, we have 90 samples available from 15 subjects. Each sample has 2 values, namely heart rate (HR) and RMSSD values over 5-minute ECG intervals. The labels file also includes 2 values, the first being the subject ID and the other the binary label (0: baseline, 1: stress). We can now proceed to split the data into a training and test set.\n",
    "\n",
    "To be fair, we should not test a model on data from the same recording. Hence, we will choose a random subject to comprise our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subject = 10\n",
    "\n",
    "# TODO: Isolate the test subject's samples using np.where\n",
    "test_subject_indices = ...\n",
    "\n",
    "# Isolate the test subject's samples\n",
    "X_test = features[test_subject_indices[0]]\n",
    "y_test = labels[test_subject_indices[0], 1]\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# TODO: Isolate the training data\n",
    "train_subject_indices = ...\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, we can apply some transformations to ease model training. In our case, our feature vector is already simple, so we will just apply z-score normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply z-score normalization to the features\n",
    "mean = ...\n",
    "std = ...\n",
    "\n",
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed, we normalized the test data in the same manner as the train data, with the train data statistics. This is important since the model is going to be trained based on those statistics. It is time now to define a classifier. We are going to use an SVM model, which is a popular model choice for narrow tasks. Let us initialize an SVM model from ``scikit-learn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Let us initialize an SVM classifier\n",
    "svm = SVC(kernel='rbf', gamma=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... You can now refer back to your assignment and return to implement the second part of the notebook.\n",
    "\n",
    "In this second part, we will train the SVM classifier and discuss the experimental results. Training the model is as simple are calling the ``fit()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the SVM classifier on y_train using X_train\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training an ML model can take a substantial amount of time, our dataset is relatively small, hence training finished almost instantly. Let's see how it performs on the unseen subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    # TODO: write a simple function to calculate accuracy %\n",
    "    acc = ...\n",
    "    return acc\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"True labels:\", y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy(y_test, predictions), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should everything work fine, you should get absolute correct predictions! This seems to be a very robust classifier and can predict if subject 10 is in a baseline or stressful condition. But is this true in general? Of course, we will never be able to know. However, we should at least get an estimate by testing on all different subjects that we have available.\n",
    "\n",
    "To do that, we can apply leave-one-subject-out (LOSO) cross-validation. According to this framework, the data are groupped per subject ID and, in each round, a single subject is used for testing, whereas the rest are used for training. We can implement that neatly with ``scikit-learn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "groups = labels[:, 0]\n",
    "\n",
    "all_predictions = np.zeros(labels.shape[0])\n",
    "for train_index, test_index in logo.split(features, labels[:, 1], groups):\n",
    "\n",
    "    # TODO: Isolate the training and test data\n",
    "    X_train, X_test = ...\n",
    "    y_train, y_test = ...\n",
    "\n",
    "    # TODO: Apply z-score normalization to the features\n",
    "    mean = ...\n",
    "    std = ...\n",
    "    X_train = ...\n",
    "    X_test = ...\n",
    "\n",
    "    # TODO: Fit the SVM classifier on y_train using X_train\n",
    "    ...\n",
    "\n",
    "    # TODO: Make predictions using the trained SVM\n",
    "    predictions = ...\n",
    "    all_predictions[test_index] = predictions\n",
    "\n",
    "print(\"Overall accuracy:\", accuracy(labels[:, 1], all_predictions), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you interpret this result? What would you change in the setup? In specific, how would you explain the model performance in terms of:\n",
    "* label ratio (class imbalance)\n",
    "* betweeen-subjects variability\n",
    "\n",
    "You can use additional cells to check those aspects of the experiment. Classification accuracy is usually not the most representative way of assesing the capabilities of a model. Below you can compute additional useful metrics that may assist you in analyzing the SVM model outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main variables of classification performance:\n",
    "true_positives = np.sum((labels[:, 1] == 1) & (all_predictions == 1))\n",
    "false_positives = np.sum((labels[:, 1] == 0) & (all_predictions == 1))\n",
    "true_negatives = np.sum((labels[:, 1] == 0) & (all_predictions == 0))\n",
    "false_negatives = np.sum((labels[:, 1] == 1) & (all_predictions == 0))\n",
    "\n",
    "print(\"True positives:\", true_positives)\n",
    "print(\"False positives:\", false_positives)\n",
    "print(\"True negatives:\", true_negatives)\n",
    "print(\"False negatives:\", false_negatives)\n",
    "\n",
    "# TODO: Calculate the following performance metrics\n",
    "\n",
    "# Precision: ratio of correctly predicted positive observations to the total predicted positives\n",
    "precision = ...\n",
    "\n",
    "# Recall: ratio of correctly predicted positive observations to all observations of this class\n",
    "recall = ...\n",
    "\n",
    "# F1 score: weighted average of precision and recall\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "# TODO: In this domain, researchers are often interested in sensitivity and specificity\n",
    "\n",
    "# Sensitivity: ratio of correctly predicted positive observations to all observations of this class\n",
    "sensitivity = ...\n",
    "\n",
    "# Specificity: ratio of correctly predicted negative observations to all observations of this class\n",
    "specificity = ...\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain what aspect of the classification task does each metric indicate? Keep notes so that you can discuss them in our meeting.\n",
    "\n",
    "#### Thanks for completing the assignment!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
